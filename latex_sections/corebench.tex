\subsection{CORE-Bench}\label{app:corebench}

\paragraph{Benchmark.}
CORE-Bench evaluates the ability of agents to computationally reproduce the results of published scientific papers. In CORE-Bench Medium, the agent is given a Dockerfile and instructions on how to use the Dockerfile to fully reproduce the paper. This level mainly evaluates agents ability to use and interact with the terminal. The agent must then answer questions about the output of the code.
Paper: \cite{corebench}.

\paragraph{Agents.}
Briefly describe the two agents you ran (scaffolds, models, reasoning settings).


\begin{table}[t]
  \centering
  \caption{CORE-Bench Leaderboard (verbatim from the website).}
  \label{tab:corebench_full}
  \input{tables/corebench_hard_full_results.tex}
\end{table}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{core_pareto_accuracy_vs_cost.pdf}
  \caption{Pareto frontier of accuracy vs.\ cost.}
  \label{fig:core_pareto}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{core_total_tokens.pdf}
  \caption{Total completion tokens used per Agent}
  \label{fig:core_tokens}
\end{figure}

\begin{figure*}[t]
  \centering
  \adjustbox{max width=\textwidth, max height=0.9\textheight}{%
    \includegraphics{core_heatmap_best_vs_any.pdf}%
  }
  \caption{Heatmap: best-agent vs.\ any-agent success.}
  \label{fig:core_heatmap}
\end{figure*}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{core_accuracy_vs_release_date.pdf}
  \caption{Accuracy vs.\ model release date.}
  \label{fig:core_release}
\end{figure}

\clearpage