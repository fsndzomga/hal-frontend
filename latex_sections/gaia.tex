\subsection{GAIA}\label{app:gaia}

\paragraph{Benchmark.}
GAIA is a benchmark for General AI Assistants that requires a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and tool-use proficiency. It contains 450 questions with unambiguous answers, requiring different levels of tooling and autonomy to solve. It is divided in 3 levels, where level 1 should be breakable by very good LLMs, and level 3 indicate a strong jump in model capabilities. We evaluate on the public validation set of 165 questions.
Paper: \cite{gaia}.

\paragraph{Agents.}
Briefly describe the two agents you ran (scaffolds, models, reasoning settings).


\begin{table}[t]
  \centering
  \caption{GAIA Leaderboard (verbatim from the website).}
  \label{tab:gaia_full}
  \input{tables/gaia_full_results.tex}
\end{table}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{gaia_pareto_accuracy_vs_cost.pdf}
  \caption{Pareto frontier of accuracy vs.\ cost.}
  \label{fig:gaia_pareto}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{gaia_total_tokens.pdf}
  \caption{Total completion tokens used per Agent}
  \label{fig:gaia_tokens}
\end{figure}

\begin{figure*}[t]
  \centering
  \adjustbox{max width=\textwidth, max height=0.9\textheight}{%
    \includegraphics{gaia_heatmap_best_vs_any.pdf}%
  }
  \caption{Heatmap: best-agent vs.\ any-agent success.}
  \label{fig:gaia_heatmap}
\end{figure*}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{gaia_accuracy_vs_release_date.pdf}
  \caption{Accuracy vs.\ model release date.}
  \label{fig:gaia_release}
\end{figure}

\clearpage